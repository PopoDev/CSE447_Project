{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "460e2fe9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-10T20:27:40.061352Z",
     "iopub.status.busy": "2024-03-10T20:27:40.061015Z",
     "iopub.status.idle": "2024-03-10T20:27:40.918488Z",
     "shell.execute_reply": "2024-03-10T20:27:40.917418Z"
    },
    "papermill": {
     "duration": 0.865617,
     "end_time": "2024-03-10T20:27:40.920803",
     "exception": false,
     "start_time": "2024-03-10T20:27:40.055186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/cse447-models/model.safetensors\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "552f79a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T20:27:40.930990Z",
     "iopub.status.busy": "2024-03-10T20:27:40.930604Z",
     "iopub.status.idle": "2024-03-10T20:27:43.809779Z",
     "shell.execute_reply": "2024-03-10T20:27:43.807964Z"
    },
    "papermill": {
     "duration": 2.887761,
     "end_time": "2024-03-10T20:27:43.813112",
     "exception": false,
     "start_time": "2024-03-10T20:27:40.925351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CSE447_Project'...\r\n",
      "remote: Enumerating objects: 323, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (62/62), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (40/40), done.\u001b[K\r\n",
      "remote: Total 323 (delta 29), reused 47 (delta 20), pack-reused 261\u001b[K\r\n",
      "Receiving objects: 100% (323/323), 10.98 MiB | 13.15 MiB/s, done.\r\n",
      "Resolving deltas: 100% (175/175), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/PopoDev/CSE447_Project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b9d305",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T20:27:43.831667Z",
     "iopub.status.busy": "2024-03-10T20:27:43.830818Z",
     "iopub.status.idle": "2024-03-10T20:27:44.875088Z",
     "shell.execute_reply": "2024-03-10T20:27:44.873736Z"
    },
    "papermill": {
     "duration": 1.056566,
     "end_time": "2024-03-10T20:27:44.877753",
     "exception": false,
     "start_time": "2024-03-10T20:27:43.821187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mCSE447_Project\u001b[0m/  __notebook__.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fec0712b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T20:27:44.891235Z",
     "iopub.status.busy": "2024-03-10T20:27:44.890296Z",
     "iopub.status.idle": "2024-03-10T20:27:44.897303Z",
     "shell.execute_reply": "2024-03-10T20:27:44.896181Z"
    },
    "papermill": {
     "duration": 0.016281,
     "end_time": "2024-03-10T20:27:44.899608",
     "exception": false,
     "start_time": "2024-03-10T20:27:44.883327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/CSE447_Project\n"
     ]
    }
   ],
   "source": [
    "cd CSE447_Project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c66ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T20:27:44.911461Z",
     "iopub.status.busy": "2024-03-10T20:27:44.911152Z",
     "iopub.status.idle": "2024-03-10T20:27:46.121381Z",
     "shell.execute_reply": "2024-03-10T20:27:46.120228Z"
    },
    "papermill": {
     "duration": 1.218734,
     "end_time": "2024-03-10T20:27:46.123717",
     "exception": false,
     "start_time": "2024-03-10T20:27:44.904983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71927d7",
   "metadata": {
    "papermill": {
     "duration": 0.005081,
     "end_time": "2024-03-10T20:27:46.134280",
     "exception": false,
     "start_time": "2024-03-10T20:27:46.129199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89db4e4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T20:27:46.146293Z",
     "iopub.status.busy": "2024-03-10T20:27:46.145954Z",
     "iopub.status.idle": "2024-03-10T20:27:46.152065Z",
     "shell.execute_reply": "2024-03-10T20:27:46.151171Z"
    },
    "papermill": {
     "duration": 0.014647,
     "end_time": "2024-03-10T20:27:46.154070",
     "exception": false,
     "start_time": "2024-03-10T20:27:46.139423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/CSE447_Project/Student\n"
     ]
    }
   ],
   "source": [
    "cd /kaggle/working/CSE447_Project/Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39d5531b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T20:27:46.165254Z",
     "iopub.status.busy": "2024-03-10T20:27:46.164767Z",
     "iopub.status.idle": "2024-03-10T20:27:47.173504Z",
     "shell.execute_reply": "2024-03-10T20:27:47.172585Z"
    },
    "papermill": {
     "duration": 1.016746,
     "end_time": "2024-03-10T20:27:47.175845",
     "exception": false,
     "start_time": "2024-03-10T20:27:46.159099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments.py  \u001b[0m\u001b[01;34mdataloader\u001b[0m/  requirements.txt  run_sbert.py  \u001b[01;34mutils\u001b[0m/\r\n",
      "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mmodel\u001b[0m/       run.py            train.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0887253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T20:27:47.188432Z",
     "iopub.status.busy": "2024-03-10T20:27:47.188130Z",
     "iopub.status.idle": "2024-03-10T20:28:03.294486Z",
     "shell.execute_reply": "2024-03-10T20:28:03.293268Z"
    },
    "papermill": {
     "duration": 16.115409,
     "end_time": "2024-03-10T20:28:03.296922",
     "exception": false,
     "start_time": "2024-03-10T20:27:47.181513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers (from -r requirements.txt (line 1))\r\n",
      "  Downloading sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting evaluate (from -r requirements.txt (line 2))\r\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (4.37.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (4.66.1)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (2.1.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (1.24.4)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (1.2.2)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (1.11.4)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (0.20.3)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (9.5.0)\r\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 2)) (2.1.0)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 2)) (0.3.7)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 2)) (2.1.4)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 2)) (2.31.0)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 2)) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 2)) (0.70.15)\r\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate->-r requirements.txt (line 2)) (2023.12.2)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 2)) (21.3)\r\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 2)) (0.18.0)\r\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 2)) (11.0.0)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 2)) (3.9.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers->-r requirements.txt (line 1)) (3.13.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers->-r requirements.txt (line 1)) (6.0.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers->-r requirements.txt (line 1)) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate->-r requirements.txt (line 2)) (3.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 2)) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 2)) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 2)) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 2)) (2023.11.17)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 1)) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 1)) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 1)) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers->-r requirements.txt (line 1)) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers->-r requirements.txt (line 1)) (0.15.1)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers->-r requirements.txt (line 1)) (0.4.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate->-r requirements.txt (line 2)) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate->-r requirements.txt (line 2)) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate->-r requirements.txt (line 2)) (2023.4)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 1)) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 1)) (3.2.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 2)) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 2)) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 2)) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 2)) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 2)) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 2)) (4.0.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate->-r requirements.txt (line 2)) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 1)) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 1)) (1.3.0)\r\n",
      "Downloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: sentence-transformers, evaluate\r\n",
      "Successfully installed evaluate-0.4.1 sentence-transformers-2.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d64989d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T20:28:03.314516Z",
     "iopub.status.busy": "2024-03-10T20:28:03.313765Z",
     "iopub.status.idle": "2024-03-10T20:28:08.902978Z",
     "shell.execute_reply": "2024-03-10T20:28:08.901725Z"
    },
    "papermill": {
     "duration": 5.600379,
     "end_time": "2024-03-10T20:28:08.905379",
     "exception": false,
     "start_time": "2024-03-10T20:28:03.305000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 10 20:28:08 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   35C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   35C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|  No running processes found                                                           |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "model = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a549940",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T20:28:08.927506Z",
     "iopub.status.busy": "2024-03-10T20:28:08.926362Z",
     "iopub.status.idle": "2024-03-10T20:55:41.398278Z",
     "shell.execute_reply": "2024-03-10T20:55:41.397182Z"
    },
    "papermill": {
     "duration": 1652.487599,
     "end_time": "2024-03-10T20:55:41.400964",
     "exception": false,
     "start_time": "2024-03-10T20:28:08.913365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-10 20:28:17.167160: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-03-10 20:28:17.167294: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-03-10 20:28:17.324306: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "tokenizer_config.json: 100%|██████████████████| 52.0/52.0 [00:00<00:00, 238kB/s]\r\n",
      "config.json: 100%|█████████████████████████████| 580/580 [00:00<00:00, 2.80MB/s]\r\n",
      "spm.model: 100%|███████████████████████████| 2.46M/2.46M [00:00<00:00, 27.2MB/s]\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\r\n",
      "  warnings.warn(\r\n",
      "pytorch_model.bin: 100%|██████████████████████| 874M/874M [00:04<00:00, 211MB/s]\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n",
      "  return self.fget.__get__(instance, owner)()\r\n",
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "<bound method Module.parameters of DebertaV2ForMultipleChoice(\r\n",
      "  (deberta): DebertaV2Model(\r\n",
      "    (embeddings): DebertaV2Embeddings(\r\n",
      "      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\r\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\r\n",
      "      (dropout): StableDropout()\r\n",
      "    )\r\n",
      "    (encoder): DebertaV2Encoder(\r\n",
      "      (layer): ModuleList(\r\n",
      "        (0-23): 24 x DebertaV2Layer(\r\n",
      "          (attention): DebertaV2Attention(\r\n",
      "            (self): DisentangledSelfAttention(\r\n",
      "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "              (pos_dropout): StableDropout()\r\n",
      "              (dropout): StableDropout()\r\n",
      "            )\r\n",
      "            (output): DebertaV2SelfOutput(\r\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\r\n",
      "              (dropout): StableDropout()\r\n",
      "            )\r\n",
      "          )\r\n",
      "          (intermediate): DebertaV2Intermediate(\r\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\r\n",
      "            (intermediate_act_fn): GELUActivation()\r\n",
      "          )\r\n",
      "          (output): DebertaV2Output(\r\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\r\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\r\n",
      "            (dropout): StableDropout()\r\n",
      "          )\r\n",
      "        )\r\n",
      "      )\r\n",
      "      (rel_embeddings): Embedding(512, 1024)\r\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\r\n",
      "    )\r\n",
      "  )\r\n",
      "  (pooler): ContextPooler(\r\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "    (dropout): StableDropout()\r\n",
      "  )\r\n",
      "  (classifier): Linear(in_features=1024, out_features=1, bias=True)\r\n",
      "  (dropout): StableDropout()\r\n",
      ")>\r\n",
      "{'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': None, 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': 0, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'microsoft/deberta-v3-large', 'transformers_version': '4.37.0', 'model_type': 'deberta-v2', 'position_buckets': 256, 'norm_rel_ebd': 'layer_norm', 'share_att_key': True, 'hidden_size': 1024, 'num_hidden_layers': 24, 'num_attention_heads': 16, 'intermediate_size': 4096, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 512, 'type_vocab_size': 0, 'initializer_range': 0.02, 'relative_attention': True, 'max_relative_positions': -1, 'position_biased_input': False, 'pos_att_type': ['p2c', 'c2p'], 'vocab_size': 128100, 'layer_norm_eps': 1e-07, 'pooler_hidden_size': 1024, 'pooler_dropout': 0, 'pooler_hidden_act': 'gelu'}\r\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-996a0fe2b4fb940c/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\r\n",
      "Downloading data files: 100%|███████████████████| 3/3 [00:00<00:00, 8879.97it/s]\r\n",
      "Extracting data files: 100%|████████████████████| 3/3 [00:00<00:00, 1523.17it/s]\r\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-996a0fe2b4fb940c/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\r\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 350.17it/s]\r\n",
      "Train data sample: {'id': '7-980', 'question_stem': 'The sun is responsible for', 'choices': {'text': ['puppies learning new tricks', 'children growing up and getting old', 'flowers wilting in a vase', 'plants sprouting, blooming and wilting']}, 'answerKey': 'D', 'facts': ['the sun is the source of energy for life on Earth', 'the sun is the source of energy for physical cycles on Earth', 'the sun is the source of solar energy called sunlight']}\r\n",
      "Loaded 4957 samples from OpenBookQA dataset with facts from the book\r\n",
      "Loaded 500 samples from OpenBookQA dataset with facts from the book\r\n",
      "Loaded 500 samples from OpenBookQA dataset with facts from the book\r\n",
      "Train dataset sample: {'input_ids': tensor([[     1,    262,   2119,    269,    262,   1271,    265,    843,    270,\r\n",
      "            432,    277,   2610,    260,    279,   2119,    269,   1744,    270,\r\n",
      "              2,  15804,   1101,    353,   7604,      2,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0],\r\n",
      "        [     1,    262,   2119,    269,    262,   1271,    265,    843,    270,\r\n",
      "            432,    277,   2610,    260,    279,   2119,    269,   1744,    270,\r\n",
      "              2,    572,   1479,    322,    263,    646,    597,      2,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0],\r\n",
      "        [     1,    262,   2119,    269,    262,   1271,    265,    843,    270,\r\n",
      "            432,    277,   2610,    260,    279,   2119,    269,   1744,    270,\r\n",
      "              2,   2948, 105902,    267,    266,  19502,      2,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0],\r\n",
      "        [     1,    262,   2119,    269,    262,   1271,    265,    843,    270,\r\n",
      "            432,    277,   2610,    260,    279,   2119,    269,   1744,    270,\r\n",
      "              2,   2239,  51081,    261,  24654,    263, 105902,      2,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\r\n",
      "              0,      0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\r\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\r\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\r\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\r\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\r\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\r\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\r\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\r\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\r\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor(3)}\r\n",
      "  0%|                                                  | 0/1240 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\r\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\r\n",
      "{'loss': 1.3181, 'learning_rate': 2.9838709677419357e-05, 'epoch': 0.4}\r\n",
      "{'loss': 1.3906, 'learning_rate': 9.67741935483871e-06, 'epoch': 0.81}\r\n",
      "100%|███████████████████████████████████████| 1240/1240 [24:46<00:00,  1.03s/it]\r\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\r\n",
      "  6%|██▊                                         | 2/32 [00:01<00:15,  1.93it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 3/32 [00:02<00:21,  1.36it/s]\u001b[A\r\n",
      " 12%|█████▌                                      | 4/32 [00:03<00:23,  1.17it/s]\u001b[A\r\n",
      " 16%|██████▉                                     | 5/32 [00:04<00:24,  1.08it/s]\u001b[A\r\n",
      " 19%|████████▎                                   | 6/32 [00:05<00:25,  1.04it/s]\u001b[A\r\n",
      " 22%|█████████▋                                  | 7/32 [00:06<00:24,  1.01it/s]\u001b[A\r\n",
      " 25%|███████████                                 | 8/32 [00:07<00:24,  1.00s/it]\u001b[A\r\n",
      " 28%|████████████▍                               | 9/32 [00:08<00:23,  1.01s/it]\u001b[A\r\n",
      " 31%|█████████████▍                             | 10/32 [00:09<00:22,  1.02s/it]\u001b[A\r\n",
      " 34%|██████████████▊                            | 11/32 [00:10<00:21,  1.02s/it]\u001b[A\r\n",
      " 38%|████████████████▏                          | 12/32 [00:11<00:20,  1.03s/it]\u001b[A\r\n",
      " 41%|█████████████████▍                         | 13/32 [00:12<00:19,  1.03s/it]\u001b[A\r\n",
      " 44%|██████████████████▊                        | 14/32 [00:13<00:18,  1.03s/it]\u001b[A\r\n",
      " 47%|████████████████████▏                      | 15/32 [00:14<00:17,  1.03s/it]\u001b[A\r\n",
      " 50%|█████████████████████▌                     | 16/32 [00:15<00:16,  1.03s/it]\u001b[A\r\n",
      " 53%|██████████████████████▊                    | 17/32 [00:16<00:15,  1.04s/it]\u001b[A\r\n",
      " 56%|████████████████████████▏                  | 18/32 [00:17<00:14,  1.04s/it]\u001b[A\r\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:18<00:13,  1.03s/it]\u001b[A\r\n",
      " 62%|██████████████████████████▉                | 20/32 [00:19<00:12,  1.04s/it]\u001b[A\r\n",
      " 66%|████████████████████████████▏              | 21/32 [00:20<00:11,  1.04s/it]\u001b[A\r\n",
      " 69%|█████████████████████████████▌             | 22/32 [00:21<00:10,  1.04s/it]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:22<00:09,  1.04s/it]\u001b[A\r\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:23<00:08,  1.03s/it]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:24<00:07,  1.04s/it]\u001b[A\r\n",
      " 81%|██████████████████████████████████▉        | 26/32 [00:25<00:06,  1.04s/it]\u001b[A\r\n",
      " 84%|████████████████████████████████████▎      | 27/32 [00:27<00:05,  1.04s/it]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:28<00:04,  1.04s/it]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:29<00:03,  1.08s/it]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:30<00:02,  1.06s/it]\u001b[A\r\n",
      " 97%|█████████████████████████████████████████▋ | 31/32 [00:31<00:01,  1.05s/it]\u001b[A\r\n",
      "100%|███████████████████████████████████████████| 32/32 [00:31<00:00,  1.16it/s]\u001b[A\r\n",
      "\r\n",
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 13.5MB/s]\r\n",
      "Accuracy: {'accuracy': 0.232}\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.3862922191619873, 'eval_accuracy': 0.232, 'eval_runtime': 33.4318, 'eval_samples_per_second': 14.956, 'eval_steps_per_second': 0.957, 'epoch': 1.0}\r\n",
      "100%|███████████████████████████████████████| 1240/1240 [25:20<00:00,  1.03s/it]\r\n",
      "100%|███████████████████████████████████████████| 32/32 [00:32<00:00,  1.16it/s]\u001b[A\r\n",
      "{'train_runtime': 1546.3999, 'train_samples_per_second': 3.206, 'train_steps_per_second': 0.802, 'train_loss': 1.3614890806136593, 'epoch': 1.0}\r\n",
      "100%|███████████████████████████████████████| 1240/1240 [25:46<00:00,  1.25s/it]\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\r\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\r\n",
      "100%|███████████████████████████████████████████| 32/32 [00:32<00:00,  1.13it/s]Accuracy: {'accuracy': 0.232}\r\n",
      "100%|███████████████████████████████████████████| 32/32 [00:32<00:00,  1.02s/it]\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\r\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\r\n",
      "100%|███████████████████████████████████████████| 32/32 [00:31<00:00,  1.19it/s]Accuracy: {'accuracy': 0.262}\r\n",
      "100%|███████████████████████████████████████████| 32/32 [00:32<00:00,  1.01s/it]\r\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1 python train.py --model \"microsoft/deberta-v3-large\" --output_dir checkpoints/deberta-large_obqa_facts1_noclues_1 --num_train_epochs 1 --use_book True --n_facts 1 --report_to=\"none\" --do_train --do_eval --per_device_train_batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08112e7",
   "metadata": {
    "papermill": {
     "duration": 0.123389,
     "end_time": "2024-03-10T20:55:41.650480",
     "exception": false,
     "start_time": "2024-03-10T20:55:41.527091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4529429,
     "sourceId": 7747977,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1685.502923,
   "end_time": "2024-03-10T20:55:42.501110",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-10T20:27:36.998187",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
